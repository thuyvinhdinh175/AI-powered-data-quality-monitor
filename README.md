# AI-Powered Data Quality Monitor

An intelligent data quality system that continuously monitors datasets for anomalies, schema drift, missing values, and suspicious patterns. It uses LLMs to provide explanations, auto-generated test cases, and remediation suggestions.

## ğŸ—ï¸ System Architecture

```
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚        Data Ingestion        â”‚
            â”‚       (CSV, API, etc.)       â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚             Validator Layer        â”‚
         â”‚        (Great Expectations +       â”‚
         â”‚            Custom Checks)          â”‚
         â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
               â†“                         â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚      Rule-based DQ     â”‚   â”‚      LLM-based Insight â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   (LangChain +         â”‚
                â†“               â”‚    DeepSeek/LLM)       â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚      Alerts       â”‚                 â†“
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â†“                 â”‚  Suggest Fixes / Logs  â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚     Dashboard     â”‚
     â”‚   (Streamlit /    â”‚
     â”‚      Superset)    â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

## ğŸ› ï¸ Core Stack

| Layer | Tools |
|-------|-------|
| Ingestion | Airflow / Pandas / Spark |
| Validation | Great Expectations + custom rules |
| LLM Insight | LangChain + DeepSeek / OpenAI / Ollama |
| Storage | S3 / Parquet / Delta |
| Visualization | Streamlit / Superset |
| Alerting | Email, Slack, or webhook |
| CI/CD | Docker + GitHub Actions |

## ğŸ” Key Features

### âœ… Data Validation Rules
* Missing values, null rate
* Schema drift / column type change
* Value range checks, duplicates

### âœ… LLM-Powered Insights
* "Why is this failing?"
* "What does this anomaly mean?"
* "Suggest a better threshold or check"

### âœ… Auto-Test Suggestion
* LLM generates Great Expectations YAML config based on data

### âœ… Alerting System
* Slack/email alerts on validation failures
* Summary logs + human-readable diagnosis

### âœ… Dashboard
* Streamlit UI to explore:
   * Daily DQ report
   * Failing checks
   * Fix suggestions
   * Anomaly charts

## ğŸš€ Advanced Ideas

| Feature | Description |
|---------|-------------|
| ğŸ§  Memory | Store historical DQ trends to detect drift over time |
| ğŸ” Active Learning | Flagged rows get reviewed by users, improving LLM fix recommendations |
| ğŸ“¥ REST API | Expose validation pipeline as an API for plug-and-play use |

## ğŸ—‚ï¸ Project Folder Structure

```
ai-data-quality-monitor/
â”‚
â”œâ”€â”€ dags/                  # Airflow DAGs (optional)
â”œâ”€â”€ data/                  # Sample data files
â”œâ”€â”€ expectations/          # Great Expectations config and suites
â”œâ”€â”€ llm_agent/             # LLM logic: LangChain chains, fix suggestor
â”œâ”€â”€ app/                   # Streamlit app code
â”œâ”€â”€ tests/                 # Unit tests for checks
â”œâ”€â”€ docker/                # Dockerfiles, entrypoints
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .github/workflows/     # CI/CD pipelines
```

## ğŸ“¦ Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/ai-data-quality-monitor.git
cd ai-data-quality-monitor

# Create and activate virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Set up Great Expectations
great_expectations init

# Run the Streamlit app
cd app
streamlit run dashboard.py
```

## ğŸš€ Usage

1. Place your datasets in the `data/` directory
2. Configure validation rules in the `expectations/` directory
3. Run the data quality checks:
   ```
   python run_validator.py --dataset your_data.csv
   ```
4. View results in the Streamlit dashboard:
   ```
   cd app
   streamlit run dashboard.py
   ```

## ğŸ”§ Configuration

- LLM settings can be configured in `config.yaml`
- Alert thresholds and rules can be modified in `expectations/rules.yaml`