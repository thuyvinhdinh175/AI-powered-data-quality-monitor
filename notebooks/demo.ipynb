{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Powered Data Quality Monitor Demo\n",
    "\n",
    "This notebook demonstrates how to use the AI-powered data quality monitor to check data quality, get LLM-generated insights, and suggest fixes for data quality issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's set up our environment and import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Add the project root to the path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import the necessary modules\n",
    "from app.validator.run_checks import run_validation, DataQualityValidator\n",
    "from app.data_ingestion.ingest import ingest\n",
    "from llm_agent.insight_generator import generate_llm_insights\n",
    "from llm_agent.fix_suggestor import suggest_fixes\n",
    "from llm_agent.expectation_generator import generate_expectations_config, analyze_dataset\n",
    "\n",
    "# Set up OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"  # Replace with your actual API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Examine Sample Data\n",
    "\n",
    "Let's load our sample transactions dataset and examine it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to our sample data\n",
    "data_path = \"../data/transactions.csv\"\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Display the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our dataset has some data quality issues:\n",
    "- Missing values in customer_id and transaction_date\n",
    "- Non-numeric values in amount ('null', 'abc')\n",
    "- Negative values in amount\n",
    "\n",
    "Let's use our data quality monitor to detect these issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Auto-generate a Great Expectations Test Suite\n",
    "\n",
    "We can use the LLM agent to automatically generate a Great Expectations test suite based on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the dataset\n",
    "dataset_info = analyze_dataset(data_path)\n",
    "print(json.dumps(dataset_info, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a Great Expectations test suite\n",
    "output_path = \"../expectations/auto_generated_suite.yml\"\n",
    "config = generate_expectations_config(data_path, output_path)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Data Quality Validation\n",
    "\n",
    "Let's run the data quality validation using our existing expectation suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run validation using the transactions_suite\n",
    "suite_name = \"transactions_suite\"\n",
    "validation_results = run_validation(data_path, suite_name)\n",
    "\n",
    "# Display validation results summary\n",
    "print(f\"Validation success: {validation_results['success']}\")\n",
    "print(f\"Total checks: {validation_results['statistics']['evaluated_expectations']}\")\n",
    "print(f\"Passed checks: {validation_results['statistics']['successful_expectations']}\")\n",
    "print(f\"Failed checks: {validation_results['statistics']['unsuccessful_expectations']}\")\n",
    "print(f\"Success rate: {validation_results['statistics']['success_percent']}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display details of failed checks\n",
    "for i, check in enumerate(validation_results.get('failed_checks', [])):\n",
    "    print(f\"\\nFailed Check #{i+1}: {check['check_name']}\")\n",
    "    print(f\"  Check Type: {check['check_type']}\")\n",
    "    print(f\"  Failed Rows: {check['failed_rows']} ({check['failure_percentage']}%)\")\n",
    "    print(f\"  Expected Value: {check['expected_value']}\")\n",
    "    print(f\"  Actual Value: {check['actual_value']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate LLM Insights\n",
    "\n",
    "Let's use the LLM agent to generate insights for the failed checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the path to the validation results file\n",
    "results_path = f\"../data/validation_results/{datetime.now().strftime('%Y-%m-%d')}/transactions/results.json\"\n",
    "\n",
    "# Generate insights\n",
    "insights = generate_llm_insights(results_path)\n",
    "\n",
    "# Display insights\n",
    "for check_name, insight in insights.items():\n",
    "    print(f\"\\nInsight for {check_name}:\")\n",
    "    print(f\"  Issue Description: {insight.get('issue_description')}\")\n",
    "    print(f\"  Impact Level: {insight.get('impact_level')}\")\n",
    "    print(f\"  Business Impact: {insight.get('business_impact')}\")\n",
    "    \n",
    "    print(\"  Possible Causes:\")\n",
    "    for cause in insight.get('possible_causes', []):\n",
    "        print(f\"    - {cause}\")\n",
    "    \n",
    "    print(\"  Recommended Actions:\")\n",
    "    for action in insight.get('recommended_actions', []):\n",
    "        print(f\"    - {action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Fix Suggestions\n",
    "\n",
    "Now let's use the LLM agent to suggest fixes for the failed checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate fix suggestions\n",
    "fixes = suggest_fixes(results_path)\n",
    "\n",
    "# Display fix suggestions\n",
    "for check_name, fix in fixes.items():\n",
    "    print(f\"\\nFix Suggestion for {check_name}:\")\n",
    "    print(f\"  Approach: {fix.get('fix_approach')}\")\n",
    "    print(f\"  Rationale: {fix.get('rationale')}\")\n",
    "    print(f\"  Confidence: {fix.get('confidence')}\")\n",
    "    \n",
    "    print(\"  Implementation:\")\n",
    "    print(f\"```\\n{fix.get('implementation')}\\n```\")\n",
    "    \n",
    "    print(\"  Alternative Approaches:\")\n",
    "    for alt in fix.get('alternative_approaches', []):\n",
    "        print(f\"    - {alt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implement a Fix and Revalidate\n",
    "\n",
    "Let's implement one of the suggested fixes and revalidate the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a fix for the dataset\n",
    "def fix_dataset(df):\n",
    "    # 1. Handle missing values\n",
    "    df['customer_id'].fillna('UNKNOWN', inplace=True)\n",
    "    \n",
    "    # 2. Handle non-numeric values in amount\n",
    "    df['amount'] = pd.to_numeric(df['amount'], errors='coerce')\n",
    "    \n",
    "    # 3. Fill missing amounts with the mean\n",
    "    mean_amount = df['amount'].mean()\n",
    "    df['amount'].fillna(mean_amount, inplace=True)\n",
    "    \n",
    "    # 4. Handle negative amounts - convert to positive and mark as refund\n",
    "    refunds = df['amount'] < 0\n",
    "    df.loc[refunds, 'status'] = 'Refunded'\n",
    "    df.loc[refunds, 'amount'] = df.loc[refunds, 'amount'].abs()\n",
    "    \n",
    "    # 5. Convert transaction_date to proper format\n",
    "    df['transaction_date'] = pd.to_datetime(df['transaction_date'], errors='coerce')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the fix\n",
    "df_fixed = fix_dataset(df.copy())\n",
    "\n",
    "# Save the fixed dataset\n",
    "fixed_data_path = \"../data/transactions_fixed.csv\"\n",
    "df_fixed.to_csv(fixed_data_path, index=False)\n",
    "\n",
    "# Display the fixed dataset\n",
    "df_fixed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revalidate the fixed dataset\n",
    "fixed_validation_results = run_validation(fixed_data_path, suite_name)\n",
    "\n",
    "# Display validation results summary\n",
    "print(f\"Validation success: {fixed_validation_results['success']}\")\n",
    "print(f\"Total checks: {fixed_validation_results['statistics']['evaluated_expectations']}\")\n",
    "print(f\"Passed checks: {fixed_validation_results['statistics']['successful_expectations']}\")\n",
    "print(f\"Failed checks: {fixed_validation_results['statistics']['unsuccessful_expectations']}\")\n",
    "print(f\"Success rate: {fixed_validation_results['statistics']['success_percent']}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated the core functionality of the AI-powered data quality monitor:\n",
    "\n",
    "1. Auto-generating test suites based on data\n",
    "2. Validating datasets against expectations\n",
    "3. Generating LLM insights for failed checks\n",
    "4. Suggesting fixes for data quality issues\n",
    "5. Implementing fixes and revalidating\n",
    "\n",
    "This system helps data teams identify and resolve data quality issues more efficiently by combining traditional rule-based validation with AI-powered insights and recommendations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
